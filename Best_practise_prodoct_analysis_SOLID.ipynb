{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "3"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 1. DRY: Don't repeat yourself\n",
    "<p><img src=\"https://assets.datacamp.com/production/project_1234/img/survey.png\" style=\"float: center;\" alt=\"A pictogram of a blood bag with blood donation written in it\" width=\"100%\"></p>\n",
    "<p>Have you ever started your data analysis and ended up with repetitive code? Our colleague Brenda who works as a Product Analyst, has found herself in this situation and has asked us for some help. She's written a script to pull Net Promotor Score (NPS) data from various sources. NPS works by asking <em>How likely is it that you would recommend our product to a friend or colleague?</em> with a rating scale of 0 to 10. Brenda has set up this NPS survey in various ways, including emails and pop-ups on the mobile app and website. To compile the data from the different sources, she's written the following code:</p>\n",
    "<pre><code class=\"py language-py\"># Read the NPS email responses into a DataFrame\n",
    "email = pd.read_csv(\"datasets/2020Q4_nps_email.csv\")\n",
    "# Add a column to record the source\n",
    "email['source'] = 'email'\n",
    "\n",
    "# Repeat for NPS mobile and web responses\n",
    "mobile = pd.read_csv(\"datasets/2020Q4_nps_mobile.csv\")\n",
    "mobile['source'] = 'mobile'\n",
    "web = pd.read_csv(\"datasets/2020Q4_nps_web.csv\")\n",
    "web['source'] = 'web'\n",
    "\n",
    "# Combine the DataFrames\n",
    "q4_nps = pd.concat([email,mobile,web])\n",
    "</code></pre>\n",
    "<p>This results in the DataFrame <code>q4_nps</code> that looks like this:</p>\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th></th>\n",
    "<th><code>response_date</code></th>\n",
    "<th><code>user_id</code></th>\n",
    "<th><code>nps_rating</code></th>\n",
    "<th><code>source</code></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>2020-10-29</td>\n",
    "<td>36742</td>\n",
    "<td>2</td>\n",
    "<td>email</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>2020-11-26</td>\n",
    "<td>31851</td>\n",
    "<td>10</td>\n",
    "<td>email</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>2020-10-27</td>\n",
    "<td>44299</td>\n",
    "<td>10</td>\n",
    "<td>email</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>…</td>\n",
    "<td>…</td>\n",
    "<td>…</td>\n",
    "<td>…</td>\n",
    "<td>…</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "<p>This code works, but it violates the Don't Repeat Yourself (DRY) programming principle. Brenda repeats the same code for email, mobile, and web, except with different variable names and file names. While it's often quicker to copy and paste, it makes it easier to introduce errors. For example, if you need to edit one of those lines, you have to do it in multiple places. Enter functions! Repeated code is a sign that we need functions. Let's write a function for Brenda.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "dc": {
     "key": "3"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>nps_rating</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>14178</td>\n",
       "      <td>3</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>33221</td>\n",
       "      <td>1</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>21127</td>\n",
       "      <td>10</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-07</td>\n",
       "      <td>42894</td>\n",
       "      <td>3</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-26</td>\n",
       "      <td>30501</td>\n",
       "      <td>5</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>49529</td>\n",
       "      <td>3</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>23671</td>\n",
       "      <td>7</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>2020-11-28</td>\n",
       "      <td>39954</td>\n",
       "      <td>7</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>2020-12-19</td>\n",
       "      <td>21098</td>\n",
       "      <td>7</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>14919</td>\n",
       "      <td>7</td>\n",
       "      <td>mobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1801 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     response_date  user_id  nps_rating  source\n",
       "0       2020-12-29    14178           3  mobile\n",
       "1       2020-10-29    33221           1  mobile\n",
       "2       2020-11-01    21127          10  mobile\n",
       "3       2020-12-07    42894           3  mobile\n",
       "4       2020-11-26    30501           5  mobile\n",
       "...            ...      ...         ...     ...\n",
       "1796    2020-12-29    49529           3  mobile\n",
       "1797    2020-12-24    23671           7  mobile\n",
       "1798    2020-11-28    39954           7  mobile\n",
       "1799    2020-12-19    21098           7  mobile\n",
       "1800    2020-12-23    14919           7  mobile\n",
       "\n",
       "[1801 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#s\n",
    "# Import pandas with the usual alias\n",
    "import pandas as pd\n",
    "\n",
    "# Write a function that matches the docstring\n",
    "def convert_csv_to_df(csv_name, source_type):\n",
    "    \"\"\" Converts an NPS CSV into a DataFrame with a column for the source. \n",
    "\n",
    "    Args:\n",
    "        csv_name (str): The name of the NPS CSV file.\n",
    "        source_type (str): The source of the NPS responses.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame with the CSV data and a column, source.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_name)#1\n",
    "    df['source'] = source_type#2\n",
    "    return df\n",
    "\n",
    "# Test the function on the mobile data\n",
    "convert_csv_to_df(\"datasets/2020Q4_nps_mobile.csv\", \"mobile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "10"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 2. Verifying the files with the \"with\" keyword\n",
    "<p>Excellent, we have a function that reads and cleans Brenda's CSVs precisely the way she needs! She can call this function in the future for as many different sources as she wants. Before we combine the NPS DataFrames, we want to add a function that verifies that the files inputted are valid. Each of these NPS dataset files should have three columns: <code>response_date</code>, <code>user_id</code>, <code>nps_rating</code>. Previously, Brenda would check this manually by opening each file. </p>\n",
    "<p>Let's write a function that uses the <strong>context manager</strong> <code>with open()</code> so that we properly close the files we open, <a href=\"https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files\">even if an exception is raised</a>. If we don't use the <code>with</code> keyword with <code>open()</code> , we would need to call <code>close()</code> after we're done with the file. Even then, it's risky because an error might be raised before the <code>close()</code> functions are called. </p>\n",
    "<p>The function will return <code>True</code> if the file contains the right columns. Otherwise, it will return <code>False</code>. To test the function, we'll use <code>datasets/corrupted.csv</code> to simulate a corrupted invalid NPS file.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "dc": {
     "key": "10"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function check_csv which takes csv_name\n",
    "def check_csv(check_name):\n",
    "    \n",
    "    \"\"\" Checks if a CSV has three columns: response_date, user_id, nps_rating\n",
    "\n",
    "    Args:\n",
    "        csv_name (str): The name of the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        Boolean: True if the CSV is valid, False otherwise \n",
    "    \"\"\"\n",
    "    # Open csv_name as f using with open\n",
    "    with open(check_name) as f:\n",
    "        first_line = f.readline()\n",
    "        # Return true if the CSV has the three specified columns\n",
    "        if first_line == \"response_date,user_id,nps_rating\\n\":\n",
    "            return True\n",
    "        # Otherwise, return false    \n",
    "    return False\n",
    "\n",
    "\n",
    "# Test the function on a corrupted NPS file\n",
    "check_csv(\"datasets/corrupted.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "17"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 3. Putting it together with nested functions\n",
    "<p>Alright, we now have one function that verifies that the CSVs are valid and another that converts them into the DataFrame format needed by Brenda. What's left? Looking at the script, this is the last line we haven't covered: <code>q4_nps = pd.concat([email,mobile,web])</code>. We could use this line of code, but we'll see more code repetition if we get CSVs from other sources or time periods.</p>\n",
    "<p>To make sure our code is scalable, we're going to write a function called <code>combine_nps_csvs()</code> that takes in a dictionary. Python dictionaries have key:value pairs. In our case, the CSV's name and source type will be the key and value, respectively. That way, we can define a dictionary with as many NPS files as we have and run it through <code>combine_nps_csvs()</code>. For each file, we'll check that it's valid using <code>check_csv()</code>, and if it is, we'll use <code>convert_csv_to_df()</code> to convert it into a DataFrame. At the start of the function, we'll define an empty DataFrame called <code>combined</code> and everytime a CSV is succesfully converted, we'll concatenate it to <code>combined</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "dc": {
     "key": "17"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/corrupted.csv is not a valid file and will not be added.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>nps_rating</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-06</td>\n",
       "      <td>11037</td>\n",
       "      <td>7</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>34434</td>\n",
       "      <td>9</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-03</td>\n",
       "      <td>49547</td>\n",
       "      <td>8</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-04</td>\n",
       "      <td>13821</td>\n",
       "      <td>7</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>29407</td>\n",
       "      <td>9</td>\n",
       "      <td>email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>2020-12-25</td>\n",
       "      <td>10656</td>\n",
       "      <td>8</td>\n",
       "      <td>web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2286</th>\n",
       "      <td>2020-11-07</td>\n",
       "      <td>32918</td>\n",
       "      <td>10</td>\n",
       "      <td>web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2287</th>\n",
       "      <td>2020-10-16</td>\n",
       "      <td>15667</td>\n",
       "      <td>10</td>\n",
       "      <td>web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2288</th>\n",
       "      <td>2020-11-20</td>\n",
       "      <td>47153</td>\n",
       "      <td>7</td>\n",
       "      <td>web</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>47071</td>\n",
       "      <td>5</td>\n",
       "      <td>web</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6043 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     response_date  user_id  nps_rating source\n",
       "0       2020-11-06    11037           7  email\n",
       "1       2020-12-24    34434           9  email\n",
       "2       2020-12-03    49547           8  email\n",
       "3       2020-10-04    13821           7  email\n",
       "4       2020-10-23    29407           9  email\n",
       "...            ...      ...         ...    ...\n",
       "2285    2020-12-25    10656           8    web\n",
       "2286    2020-11-07    32918          10    web\n",
       "2287    2020-10-16    15667          10    web\n",
       "2288    2020-11-20    47153           7    web\n",
       "2289    2020-10-17    47071           5    web\n",
       "\n",
       "[6043 rows x 4 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function combine_nps_csvs() with the arg csvs_dict \n",
    "def combine_nps_csvs(csvs_dict):\n",
    "    \n",
    "    # Define combine as an empty DataFrame\n",
    "    combined = pd.DataFrame()\n",
    "    # Iterate over csvs_dict to get the name and source of the CSVs\n",
    "    for key,value in csvs_dict.items():\n",
    "        # Check if the csv is valid using check_csv()\n",
    "        if check_csv(key):\n",
    "            # Convert the CSV using convert_csv_to_df() and assign it to temp\n",
    "            temp = convert_csv_to_df(key,value)\n",
    "            # Concatenate combined and temp and assign it to combined\n",
    "            combined = pd.concat([combined, temp])        # If the file is not valid, print a message with the CSV's name\n",
    "        else:\n",
    "            print(key + \" is not a valid file and will not be added.\")\n",
    "    return combined \n",
    "\n",
    "my_files = {\n",
    "  \"datasets/2020Q4_nps_email.csv\": \"email\",\n",
    "  \"datasets/2020Q4_nps_mobile.csv\": \"mobile\",\n",
    "  \"datasets/2020Q4_nps_web.csv\": \"web\",\n",
    "  \"datasets/corrupted.csv\": \"social_media\"\n",
    "}\n",
    "\n",
    "# Test the function on the my_files dictionary \n",
    "combine_nps_csvs(my_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "24"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 4. Detractors, Passives, and Promoters\n",
    "<p>We've summarized our colleague's script into one function: <code>combine_nps_csvs()</code>! Let's move on to analyzing the NPS data, such as actually calculating NPS. As a reminder, NPS works by asking <em>How likely is it that you would recommend our product to a friend or colleague?</em> with a rating scale of 0 to 10.</p>\n",
    "<p>NPS ratings are categorized into three groups. Ratings between 0 to 6 are <strong>detractors</strong>, ratings between 7 to 8 are <strong>passives</strong>, and finally, ratings 9 to 10 are <strong>promoters</strong>. There's more to analyzing NPS, but remember, functions should be small in scope and should just \"do one thing\". So before we get ahead of ourselves, let's write a simple function that takes an NPS rating and categorizes it into the appropriate group.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "dc": {
     "key": "24"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'passive'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorize_nps(x):\n",
    "    \"\"\" \n",
    "    Takes a NPS rating and outputs whether it is a \"promoter\", \n",
    "    \"passive\", \"detractor\", or \"invalid\" score. \"invalid\" is\n",
    "    returned when the score is not between 0-10.\n",
    "\n",
    "    Args:\n",
    "        x: The NPS rating\n",
    "\n",
    "    Returns:\n",
    "        String: the NPS category or \"invalid\".\n",
    "    \"\"\"\n",
    "    # Write the rest of the function to match the docstring    \n",
    "    if x == 9 or x == 10:\n",
    "        return 'promoter'\n",
    "    elif x == 7 or x == 8:\n",
    "        return'passive'\n",
    "    elif x >= 0 and x <= 6:\n",
    "        return 'detractor'\n",
    "    else:\n",
    "        return \"invalid\"\n",
    "\n",
    "# Test the function \n",
    "categorize_nps(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "31"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 5. Applying our function to a DataFrame\n",
    "<p>So we have a function that takes a score and outputs which NPS response group it belongs to. It would be great to have this as a column in our NPS DataFrames, similar to the <code>source</code> column we added. Since we've modularized our code with functions, all we need to do is edit our <code>convert_cvs_to_df()</code> function and nest <code>categorize_nps()</code> into it. However, the way we'll nest <code>categorize_nps()</code> will be different than previous times. The <code>pandas</code> library has a handy function called <code>apply()</code>, which lets us apply a function to each column or row of a DataFrame. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "dc": {
     "key": "31"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response_date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>nps_rating</th>\n",
       "      <th>source</th>\n",
       "      <th>nps_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>14178</td>\n",
       "      <td>3</td>\n",
       "      <td>mobile</td>\n",
       "      <td>detractor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>33221</td>\n",
       "      <td>1</td>\n",
       "      <td>mobile</td>\n",
       "      <td>detractor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>21127</td>\n",
       "      <td>10</td>\n",
       "      <td>mobile</td>\n",
       "      <td>promoter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-07</td>\n",
       "      <td>42894</td>\n",
       "      <td>3</td>\n",
       "      <td>mobile</td>\n",
       "      <td>detractor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-26</td>\n",
       "      <td>30501</td>\n",
       "      <td>5</td>\n",
       "      <td>mobile</td>\n",
       "      <td>detractor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>49529</td>\n",
       "      <td>3</td>\n",
       "      <td>mobile</td>\n",
       "      <td>detractor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>23671</td>\n",
       "      <td>7</td>\n",
       "      <td>mobile</td>\n",
       "      <td>passive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>2020-11-28</td>\n",
       "      <td>39954</td>\n",
       "      <td>7</td>\n",
       "      <td>mobile</td>\n",
       "      <td>passive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>2020-12-19</td>\n",
       "      <td>21098</td>\n",
       "      <td>7</td>\n",
       "      <td>mobile</td>\n",
       "      <td>passive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>14919</td>\n",
       "      <td>7</td>\n",
       "      <td>mobile</td>\n",
       "      <td>passive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1801 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     response_date  user_id  nps_rating  source  nps_group\n",
       "0       2020-12-29    14178           3  mobile  detractor\n",
       "1       2020-10-29    33221           1  mobile  detractor\n",
       "2       2020-11-01    21127          10  mobile   promoter\n",
       "3       2020-12-07    42894           3  mobile  detractor\n",
       "4       2020-11-26    30501           5  mobile  detractor\n",
       "...            ...      ...         ...     ...        ...\n",
       "1796    2020-12-29    49529           3  mobile  detractor\n",
       "1797    2020-12-24    23671           7  mobile    passive\n",
       "1798    2020-11-28    39954           7  mobile    passive\n",
       "1799    2020-12-19    21098           7  mobile    passive\n",
       "1800    2020-12-23    14919           7  mobile    passive\n",
       "\n",
       "[1801 rows x 5 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_csv_to_df(csv_name, source_type):    \n",
    "    \"\"\" Convert an NPS CSV into a DataFrame with columns for the source and NPS group.\n",
    "\n",
    "    Args:\n",
    "        csv_name (str): The name of the NPS CSV file.\n",
    "        source_type (str): The source of the NPS responses.\n",
    "\n",
    "    Returns:\n",
    "         A DataFrame with the CSV data and columns: source and nps_group.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_name)\n",
    "    df['source'] = source_type\n",
    "    # Define a new column nps_group which applies categorize_nps to nps_rating\n",
    "    df['nps_group'] = df['nps_rating'].apply(categorize_nps)\n",
    "    return df\n",
    "\n",
    "# Test the updated function with mobile data\n",
    "convert_csv_to_df(\"datasets/2020Q4_nps_mobile.csv\", \"mobile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "38"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 6. Calculating the Net Promoter Score\n",
    "<p>If we hadn't broken down our code into functions earlier, we would've had to edit our code in multiple places to add a <code>nps_group</code> column, increasing the chance of introducing errors. It also helps that our functions have one responsibility keeping our code flexible and easier to edit and debug.</p>\n",
    "<p>Now we're in a good place to calculate the Net Promoter Score! This is calculated by subtracting the percentage of detractor ratings from the percentage of promoter ratings, in other words:</p>\n",
    "<p>$\n",
    "NPS = \\frac{\\text{# of Promoter Rating - # of Detractor Ratings}}{\\text{Total # of Respondents}} * 100\n",
    "$</p>\n",
    "<p>We want to calculate the NPS across all sources, so we'll use <code>combine_nps_csvs()</code> from Task 3 to consolidate the source files. As expected, that will output a DataFrame which we'll use as an input for a new function we're going to write, <code>calculate_nps()</code>. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "dc": {
     "key": "38"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.995035578355122"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function calculate_nps that takes a DataFrame\n",
    "def calculate_nps(nps_df):\n",
    "    # Calculate the NPS score using the nps_group column \n",
    "    counts = nps_df['nps_group'].value_counts()\n",
    "    detractor = counts['detractor']\n",
    "    promotor = counts['promoter']\n",
    "    total = counts.sum()\n",
    "    # Return the NPS Score\n",
    "    return ((promotor-detractor)/ total) * 100\n",
    "\n",
    "my_files = {\n",
    "  \"datasets/2020Q4_nps_email.csv\": \"email\",\n",
    "  \"datasets/2020Q4_nps_web.csv\": \"web\",\n",
    "  \"datasets/2020Q4_nps_mobile.csv\": \"mobile\",\n",
    "}\n",
    "\n",
    "# Test the function on the my_files dictionary\n",
    "q4_nps = combine_nps_csvs(my_files)\n",
    "calculate_nps(q4_nps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "45"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 7. Breaking down NPS by source\n",
    "<p>Is it good to have an NPS score around 10? The worst NPS score you can get is -100 when all respondents are detractors, and the best is 100 when all respondents are promoters. Depending on the industry of your service or product, average NPS scores vary a lot. However, a negative score is a bad sign because it means you have more unhappy customers than happy customers. Typically, a score over 50 is considered excellent, and above 75 is considered best in class.</p>\n",
    "<p>Although our score is above 0, it's still on the lower end of the spectrum. The product team concludes that majorly increasing NPS across our customer base is a priority. Luckily, we have this NPS data that we can analyze more so we can find data-driven solutions. A good start would be breaking down the NPS score by the source type. For instance, if people are rating lower on the web than mobile, that's some evidence we need to improve the browser experience.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "dc": {
     "key": "45"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "email     18.596311\n",
       "mobile   -14.714048\n",
       "web       22.096070\n",
       "dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function calculate_nps_by_source that takes a DataFrame\n",
    "def calculate_nps_by_source(nps_df):\n",
    "    # Group the DataFrame by source and apply calculate_nps()\n",
    "    x = nps_df.groupby(['source']).apply(calculate_nps)\n",
    "    # Return a series with the NPS scores broken by source\n",
    "    return x\n",
    "\n",
    "\n",
    "my_files = {\n",
    "  \"datasets/2020Q4_nps_email.csv\": \"email\",\n",
    "  \"datasets/2020Q4_nps_web.csv\": \"web\",\n",
    "  \"datasets/2020Q4_nps_mobile.csv\": \"mobile\",\n",
    "}\n",
    "\n",
    "# Test the function on the my_files dictionary\n",
    "q4_nps = combine_nps_csvs(my_files)\n",
    "calculate_nps_by_source(q4_nps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "52"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 8. Adding docstrings\n",
    "<p>Interesting! The mobile responses have an NPS score of about -15, which is noticeably lower than the other two sources. There are few ways we could continue our analysis from here. We could use the column <code>user_id</code> to reach out to users who rated poorly on the mobile app for user interviews. We could also breakdown NPS by source and date to see if there was a date where NPS started clearly decreasing - perhaps the same time there was a bug or feature realeased. With the functions we created, Brenda is in a good place to continue this work!</p>\n",
    "<p>The last thing we'll discuss is docstrings. In Task 1, 2, 4 and 5, we included docstrings for <code>convert_csv_to_df()</code>, <code>check_csv()</code>, and <code>categorize_nps()</code>. However, we should include docstrings for all the functions we write so that others can better re-use and maintain our code. Docstrings tell readers what a function does, its arguments, its return value(s) if any, and any other useful information you want to include, like error handling. There are several standards for writing docstrings in Python, including: <a href=\"https://numpydoc.readthedocs.io/en/latest/format.html\">Numpydoc</a>, <a href=\"https://google.github.io/styleguide/pyguide.html\">Google-style</a> (chosen style in this notebook), and <a href=\"https://www.python.org/dev/peps/pep-0287/\">reStructuredText</a>.</p>\n",
    "<p>To make sure Brenda and other colleagues can follow our work, we are going to add docstrings to the remaining undocumented functions: <code>combine_nps_csvs()</code>, <code>calculate_nps()</code>, and <code>calculate_nps_by_source</code>. It's up to you how you want to write the docstrings - we'll only check that a docstring exists for each of these functions. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dc": {
     "key": "52"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "def combine_nps_csvs(csvs_dict):\n",
    "    \"\"\" \n",
    "    ADD DOCSTRING HERE\n",
    "    \"\"\"\n",
    "    combined = pd.DataFrame()\n",
    "    for csv_name, source_type in csvs_dict.items():\n",
    "        if check_csv(csv_name):\n",
    "            temp = convert_csv_to_df(csv_name, source_type)\n",
    "            combined = pd.concat([combined, temp])\n",
    "        else:\n",
    "            print(csv_name + \" is not a valid file and will not be added.\")\n",
    "    return combined\n",
    "\n",
    "def calculate_nps(nps_df):\n",
    "    \"\"\" \n",
    "    ADD DOCSTRING HERE\n",
    "    \"\"\"\n",
    "    counts = nps_df['nps_group'].value_counts()\n",
    "    detractor = counts['detractor']\n",
    "    promotor = counts['promoter']\n",
    "    total = counts.sum()\n",
    "    return ((promotor-detractor)/ total) * 100\n",
    "\n",
    "def calculate_nps_by_source(nps_df):\n",
    "    \"\"\" \n",
    "    ADD DOCSTRING HERE\n",
    "    \"\"\"\n",
    "    x = nps_df.groupby(['source']).apply(calculate_nps)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solid principles \n",
    "                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.single responsibility\n",
    ". Separate Data Loading and Calculations:\n",
    "\n",
    "Currently, the combine_nps_csvs method performs both data loading (reading CSV files) and data transformation (categorizing ratings).\n",
    "\n",
    "def load_nps_data(source):\n",
    "    \"\"\"\n",
    "    Loads NPS data from a source (specific implementation depends on source type).\n",
    "\n",
    "    Args:\n",
    "        source: A dictionary containing information about the NPS data source.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The raw NPS data as a DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(source, CsvNpsDataSource):\n",
    "        return source.get_nps_data()\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported NPS data source type: {type(source)}\") \n",
    "        \n",
    "and Then, the combine_nps_data method in NPSData would only be responsible\n",
    "    for iterating through sources, loading data using the load_nps_data function, and combining the resulting DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.open for extension closed for modification\n",
    "we can introduce a method to add new data sources dynamically. This way, we can add new sources (e.g., social media) without modifying the existing logic for handling sources.\n",
    "\n",
    "def add_data_source(self, csv_name, source_type):\r\n",
    "    \"\"\"\r\n",
    "    Adds a new data source to the `sources` dictionary.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        csv_name (str): The name of the NPS CSV file for the new source.\r\n",
    "        source_type (str): The type of the new data source.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    self.sources[csv_name] = sou2. Extending NPS Rating Logic:\n",
    "\n",
    "The convert_csv_to_df method categorizes NPS ratings into promoters, passives, and detractors. This logic can be extended using inheritance or composition. we might want to incorporate additional categories in the future (e.g., neutrals).\n",
    "Composition\n",
    "\n",
    "Introduce an interface NpsRatingCategorizer with a categorize method. Then, create concrete classes for different categorization schemes and inject them into the NPSData class.\n",
    "\n",
    "class NpsRatingCategorizer:\n",
    "    \"\"\"\n",
    "    Interface for NPS rating categorization logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def categorize(self, rating):\n",
    "        raise NotImplementedError(\"Subclasses must implement categorize()\")\n",
    "\n",
    "\n",
    "class StandardNpsCategorizer(NpsRatingCategorizer):\n",
    "    \"\"\"\n",
    "    Categorizes NPS ratings into promoters, passives, and detractors.\n",
    "    \"\"\"\n",
    "\n",
    "    def categorize(self, rating):\n",
    "        if rating in (9, 10):\n",
    "            return \"promoter\"\n",
    "        elif rating in (7, 8):\n",
    "            return \"passive\"\n",
    "        elif 0 <= rating <= 6:\n",
    "            return \"detractor\"\n",
    "        else:\n",
    "            return \"invalid\"\n",
    "\n",
    "\n",
    "By using composition, we can extend the NPS rating logic without modifying the core convert_csv_to_df method.rce_type\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Liskov Substitution Principle\n",
    "we can introduce a base class NpsDataSource with an abstract method get_nps_data that enforces returning a DataFrame with \n",
    "specific columns containing NPS data.\n",
    "\n",
    "class NpsDataSource:\n",
    "    \"\"\"\n",
    "    Abstract base class for NPS data sources.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_nps_data(self):\n",
    "        \"\"\"\n",
    "        Returns NPS data as a DataFrame with specific columns.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: Subclasses must implement this method.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Subclasses must implement get_nps_data()\")\n",
    "\n",
    "\n",
    "class CsvNpsDataSource(NpsDataSource):\n",
    "    \"\"\"\n",
    "    Concrete NPS data source for CSV files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_name):\n",
    "        self.csv_name = csv_name\n",
    "\n",
    "    def get_nps_data(self):\n",
    "        \"\"\"\n",
    "        Reads NPS data from a CSV file and returns a DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The NPS data DataFrame with expected columns.\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_csv(self.csv_name)\n",
    "        return df[[\"response_date\", \"user_id\", \"nps_rating\"]]  # Ensure expected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our all code is \n",
    "import pandas as pd\n",
    "\n",
    "#3 Liskov \n",
    "class NpsDataSource:\n",
    "    \"\"\"\n",
    "    Abstract base class for NPS data sources.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_nps_data(self):\n",
    "        \"\"\"\n",
    "        Returns NPS data as a DataFrame with specific columns.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: Subclasses must implement this method.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Subclasses must implement get_nps_data()\")\n",
    "\n",
    "\n",
    "class CsvNpsDataSource(NpsDataSource):\n",
    "    \"\"\"\n",
    "    Concrete NPS data source for CSV files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_name):\n",
    "        self.csv_name = csv_name\n",
    "\n",
    "    def get_nps_data(self):\n",
    "        \"\"\"\n",
    "        Reads NPS data from a CSV file and returns a DataFrame with expected columns.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The NPS data DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_csv(self.csv_name)\n",
    "        return df[[\"response_date\", \"user_id\", \"nps_rating\"]]\n",
    "\n",
    "#2 open for extensions closed for modifications \n",
    "class NpsRatingCategorizer:\n",
    "    \"\"\"\n",
    "    Interface for NPS rating categorization logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def categorize(self, rating):\n",
    "        raise NotImplementedError(\"Subclasses must implement categorize()\")\n",
    "\n",
    "    def validate_nps_group(self, nps_group):\n",
    "        \"\"\"\n",
    "        Validates if the NPS group is valid (promoter, passive, detractor, invalid).\n",
    "\n",
    "        Args:\n",
    "            nps_group (str): The NPS group to validate.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the NPS group is invalid.\n",
    "        \"\"\"\n",
    "\n",
    "        valid_groups = [\"promoter\", \"passive\", \"detractor\", \"invalid\"]\n",
    "        if nps_group not in valid_groups:\n",
    "            raise ValueError(f\"Invalid NPS group: {nps_group}\")\n",
    "\n",
    "\n",
    "class StandardNpsCategorizer(NpsRatingCategorizer):\n",
    "    \"\"\"\n",
    "    Categorizes NPS ratings into promoters, passives, and detractors.\n",
    "    \"\"\"\n",
    "\n",
    "    def categorize(self, rating):\n",
    "        if rating in (9, 10):\n",
    "            return \"promoter\"\n",
    "        elif rating in (7, 8):\n",
    "            return \"passive\"\n",
    "        elif 0 <= rating <= 6:\n",
    "            return \"detractor\"\n",
    "        else:\n",
    "            return \"invalid\"\n",
    "\n",
    "    def validate_nps_group(self, nps_group):\n",
    "        super().validate_nps_group(nps_group)\n",
    "\n",
    "# single Responsibility\n",
    "def load_nps_data(source):\n",
    "    \"\"\"\n",
    "    Loads NPS data from a source (specific implementation depends on source type).\n",
    "\n",
    "    Args:\n",
    "        source: A dictionary containing information about the NPS data source.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The raw NPS data as a DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(source, CsvNpsDataSource):\n",
    "        return source.get_nps_data()\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported NPS data source type: {type(source)}\")\n",
    "\n",
    "\n",
    "def clean_nps_data(data):\n",
    "    \"\"\"\n",
    "    Cleans an NPS data DataFrame by ensuring expected columns and handling missing values.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The NPS data DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned NPS data DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    expected_columns = [\"response_date\", \"user_id\", \"nps_rating\"]\n",
    "    if not all(col in data.columns for col in expected_columns):\n",
    "        raise ValueError(f\"Missing expected columns in NPS data: {expected_columns}\")\n",
    "    # Implement logic to handle missing values (optional)\n",
    "    return data\n",
    "\n",
    "\n",
    "class NPSData:\n",
    "    \"\"\"\n",
    "    This class represents NPS data from various sources.\n",
    "\n",
    "    Properties:\n",
    "        data (pd.DataFrame): The combined NPS data from all sources.\n",
    "        sources (dict): A dictionary mapping source names to NpsDataSource objects.\n",
    "        categorizer (NpsRatingCategorizer): The object used for NPS rating categorization.\n",
    "\n",
    "    Methods:\n",
    "        combine_nps_data(self): Combines NPS data from all sources in the `sources` dictionary.\n",
    "        calculate_nps(self): Calculates the overall NPS score.\n",
    "        calculate_nps_by_source(self): Calculates the NPS score for each source.\n",
    "        add_data_source(self, csv_name, source_type):\n",
    "            Adds a new data source to the `sources` dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sources, categorizer=StandardNpsCategorizer()):\n",
    "        self.data = None\n",
    "        self.sources = {name: source for name, source in sources.items() if isinstance(source, NpsDataSource)}\n",
    "        self.categorizer = categorizer\n",
    "#2 sr\n",
    "    def combine_nps_data(self):\n",
    "        \"\"\"\n",
    "        Combines NPS data from all sources in the `sources` dictionary.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If a source is not an NpsDataSource subclass.\n",
    "        \"\"\"\n",
    "\n",
    "        combined = pd.DataFrame()\n",
    "        for name, source in self.sources.items():\n",
    "            raw_data = load_nps_data(source)  # Use separate function for data loading\n",
    "            cleaned_data = clean_nps_data(raw_data)  # Use separate function for data cleaning\n",
    "            data_with_groups = self.convert_csv_to_df(cleaned_data, name)  # Transform cleaned data\n",
    "            combined = pd.concat([combined, data_with_groups])\n",
    "        self.data = combined\n",
    "\n",
    "    def convert_csv_to_df(self, data, source_type):\n",
    "        \"\"\"\n",
    "        Transforms cleaned NPS data by adding source and NPS group columns.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The cleaned NPS data DataFrame.\n",
    "            source_type (str): The source of the NPS responses.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The NPS data DataFrame with source and NPS group columns.\n",
    "        \"\"\"\n",
    "\n",
    "        data[\"source\"] = source_type\n",
    "        data[\"nps_group\"] = data[\"nps_rating\"].apply(self.categorizer.categorize)\n",
    "        self.categorizer.validate_nps_group(data[\"nps_group\"].iloc[0])  # Validate first NPS group\n",
    "        return data\n",
    "\n",
    "    def calculate_nps(self):\n",
    "        \"\"\"\n",
    "        Calculates the overall NPS score.\n",
    "\n",
    "        Assumes the `data` property is already populated with combined NPS data.\n",
    "\n",
    "        Returns:\n",
    "            float: The overall NPS score.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"NPS data is not yet combined. Call combine_nps_data() first.\")\n",
    "\n",
    "        counts = self.data[\"nps_group\"].value_counts()\n",
    "        detractor = counts.get(\"detractor\", 0)\n",
    "        promoter = counts.get(\"promoter\", 0)\n",
    "        total = counts.sum()\n",
    "\n",
    "        return ((promoter - detractor) / total) * 100\n",
    "\n",
    "    def calculate_nps_by_source(self):\n",
    "        \"\"\"\n",
    "        Calculates the NPS score for each source.\n",
    "\n",
    "        Assumes the `data` property is already populated with combined NPS data.\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: A Series with NPS scores for each source.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"NPS data is not yet combined. Call combine_nps_data() first.\")\n",
    "\n",
    "        return self.data.groupby([\"source\"]).apply(self.calculate_nps)\n",
    "\n",
    "    def add_data_source(self, csv_name, source_type):\n",
    "        \"\"\"\n",
    "        Adds a new data source to the `sources` dictionary.\n",
    "\n",
    "        Args:\n",
    "            csv_name (str): The name of the NPS CSV file for the new source.\n",
    "            source_type (str): The type of the new data source.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(source_type, str):\n",
    "            raise TypeError(\"source_type must be a string\")\n",
    "        self.sources[csv_name] = CsvNpsDataSource(csv_name)  # Create CsvNpsDataSource instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 Interface Segregation \n",
    "\n",
    "NpsDataSource can then inherit from both BasicNpsDataSource and a separate interface (e.g., ExtendedNpsDataSource) for functionalities like retrieving additional data or metadata.\n",
    "Separate Interface for Categorization Logic:\n",
    "The NpsRatingCategorizer interface defines both categorize (for converting ratings to groups) and validate_nps_group (for validation).\n",
    "Following ISP, we can split this interface into two:\n",
    "NpsRatingConverter: Defines only the categorize method.\n",
    "NpsGroupValidator: Defines only the validate_nps_group method.\n",
    "\n",
    "class NpsRatingConverter:\n",
    "    \"\"\"\n",
    "    Interface for NPS rating categorization.\n",
    "    \"\"\"\n",
    "\n",
    "    def categorize(self, rating):\n",
    "        raise NotImplementedError(\"Subclasses must implement categorize()\")\n",
    "\n",
    "\n",
    "class NpsGroupValidator:\n",
    "    \"\"\"\n",
    "    Interface for NPS group validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def validate_nps_group(self, nps_group):\n",
    "        raise NotImplementedError(\"Subclasses must implement validate_nps_group()\")\n",
    "\n",
    "\n",
    "class StandardNpsCategorizer(NpsRatingConverter, NpsGroupValidator):\n",
    "    \"\"\"\n",
    "    Categorizes NPS ratings into promoters, passives, and detractors.\n",
    "    \"\"\"\n",
    "\n",
    "    def categorize(self, rating):\n",
    "        if rating in (9, 10):\n",
    "            return \"promoter\"\n",
    "        elif rating in (7, 8):\n",
    "            return \"passive\"\n",
    "        elif 0 <= rating <= 6:\n",
    "            return \"detractor\"\n",
    "        else:\n",
    "            return \"invalid\"\n",
    "\n",
    "    def validate_nps_group(self, nps_group):\n",
    "        valid_groups = [\"promoter\", \"passive\", \"detractor\", \"invalid\"]\n",
    "        if nps_group not in valid_groups:\n",
    "            raise ValueError(f\"Invalid NPS group: {nps_group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5 Dependency Inversion\n",
    "Inject Categorizer and Validator:\n",
    "Currently, the NPSData class creates an instance of StandardNpsCategorizer internally.\n",
    "Following DIP, we can modify the constructor of NPSData to accept the categorizer and validator objects as arguments. This allows for dependency injection, making it easier to use different categorization and validation logic.\n",
    "Python\n",
    "class NPSData:\n",
    "    \"\"\"\n",
    "    This class represents NPS data from various sources.\n",
    "\n",
    "    Properties:\n",
    "        data (pd.DataFrame): The combined NPS data from all sources.\n",
    "        sources (dict): A dictionary mapping source names to NpsDataSource objects.\n",
    "        categorizer (NpsRatingConverter): The object used for NPS rating categorization.\n",
    "        validator (NpsGroupValidator, optional): The object used for NPS group validation.\n",
    "\n",
    "    Methods:\n",
    "        combine_nps_data(self): Combines NPS data from all sources in the `sources` dictionary.\n",
    "        calculate_nps(self): Calculates the overall NPS score.\n",
    "        calculate_nps_by_source(self): Calculates the NPS score for each source.\n",
    "        add_data_source(self, csv_name, source_type):\n",
    "            Adds a new data source to the `sources` dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sources, categorizer=StandardNpsCategorizer(), validator=None):\n",
    "        self.data = None\n",
    "        self.sources = {name: source for name, source in sources.items() if isinstance(source, NpsDataSource)}\n",
    "        self.categorizer = categorizer\n",
    "        self.validator = validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 158 (3098829479.py, line 159)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 159\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.data = None\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 158\n"
     ]
    }
   ],
   "source": [
    "#Our final code is \n",
    "# our all code is \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class NpsDataSource:\n",
    "    \"\"\"\n",
    "    Abstract base class for NPS data sources.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_nps_data(self):\n",
    "        \"\"\"\n",
    "        Returns NPS data as a DataFrame with specific columns.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: Subclasses must implement this method.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Subclasses must implement get_nps_data()\")\n",
    "\n",
    "\n",
    "class CsvNpsDataSource(NpsDataSource):\n",
    "    \"\"\"\n",
    "    Concrete NPS data source for CSV files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_name):\n",
    "        self.csv_name = csv_name\n",
    "\n",
    "    def get_nps_data(self):\n",
    "        \"\"\"\n",
    "        Reads NPS data from a CSV file and returns a DataFrame with expected columns.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The NPS data DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_csv(self.csv_name)\n",
    "        return df[[\"response_date\", \"user_id\", \"nps_rating\"]]\n",
    "\n",
    "\n",
    "class NpsRatingCategorizer:\n",
    "    \"\"\"\n",
    "    Interface for NPS rating categorization logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def categorize(self, rating):\n",
    "        raise NotImplementedError(\"Subclasses must implement categorize()\")\n",
    "\n",
    "    def validate_nps_group(self, nps_group):\n",
    "        \"\"\"\n",
    "        Validates if the NPS group is valid (promoter, passive, detractor, invalid).\n",
    "\n",
    "        Args:\n",
    "            nps_group (str): The NPS group to validate.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the NPS group is invalid.\n",
    "        \"\"\"\n",
    "\n",
    "        valid_groups = [\"promoter\", \"passive\", \"detractor\", \"invalid\"]\n",
    "        if nps_group not in valid_groups:\n",
    "            raise ValueError(f\"Invalid NPS group: {nps_group}\")\n",
    "\n",
    "\n",
    "class NpsRatingConverter:\n",
    "    \"\"\"\n",
    "    Interface for NPS rating categorization.\n",
    "    \"\"\"\n",
    "\n",
    "    def categorize(self, rating):\n",
    "        raise NotImplementedError(\"Subclasses must implement categorize()\")\n",
    "\n",
    "\n",
    "class NpsGroupValidator:\n",
    "    \"\"\"\n",
    "    Interface for NPS group validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def validate_nps_group(self, nps_group):\n",
    "        raise NotImplementedError(\"Subclasses must implement validate_nps_group()\")\n",
    "\n",
    "\n",
    "class StandardNpsCategorizer(NpsRatingConverter, NpsGroupValidator):\n",
    "    \"\"\"\n",
    "    Categorizes NPS ratings into promoters, passives, and detractors.\n",
    "    \"\"\"\n",
    "\n",
    "    def categorize(self, rating):\n",
    "        if rating in (9, 10):\n",
    "            return \"promoter\"\n",
    "        elif rating in (7, 8):\n",
    "            return \"passive\"\n",
    "        elif 0 <= rating <= 6:\n",
    "            return \"detractor\"\n",
    "        else:\n",
    "            return \"invalid\"\n",
    "\n",
    "    def validate_nps_group(self, nps_group):\n",
    "        valid_groups = [\"promoter\", \"passive\", \"detractor\", \"invalid\"]\n",
    "        if nps_group not in valid_groups:\n",
    "            raise ValueError(f\"Invalid NPS group: {nps_group}\")\n",
    "\n",
    "\n",
    "\n",
    "def load_nps_data(source):\n",
    "    \"\"\"\n",
    "    Loads NPS data from a source (specific implementation depends on source type).\n",
    "\n",
    "    Args:\n",
    "        source: A dictionary containing information about the NPS data source.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The raw NPS data as a DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(source, CsvNpsDataSource):\n",
    "        return source.get_nps_data()\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported NPS data source type: {type(source)}\")\n",
    "\n",
    "\n",
    "def clean_nps_data(data):\n",
    "    \"\"\"\n",
    "    Cleans an NPS data DataFrame by ensuring expected columns and handling missing values.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The NPS data DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned NPS data DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    expected_columns = [\"response_date\", \"user_id\", \"nps_rating\"]\n",
    "    if not all(col in data.columns for col in expected_columns):\n",
    "        raise ValueError(f\"Missing expected columns in NPS data: {expected_columns}\")\n",
    "    # Implement logic to handle missing values (optional)\n",
    "    return data\n",
    "\n",
    "\n",
    "class NPSData:\n",
    "    \"\"\"\n",
    "    This class represents NPS data from various sources.\n",
    "\n",
    "    Properties:\n",
    "        data (pd.DataFrame): The combined NPS data from all sources.\n",
    "        sources (dict): A dictionary mapping source names to NpsDataSource objects.\n",
    "        categorizer (NpsRatingCategorizer): The object used for NPS rating categorization.\n",
    "\n",
    "    Methods:\n",
    "        combine_nps_data(self): Combines NPS data from all sources in the `sources` dictionary.\n",
    "        calculate_nps(self): Calculates the overall NPS score.\n",
    "        calculate_nps_by_source(self): Calculates the NPS score for each source.\n",
    "        add_data_source(self, csv_name, source_type):\n",
    "            Adds a new data source to the `sources` dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sources, categorizer=StandardNpsCategorizer(), validator=None):\r\n",
    "    self.data = None\r\n",
    "    self.sources = {name: source for name, source in sources.items() if isinstance(source, NpsDataSource)}\r\n",
    "    self.categorizer = categorizer\r\n",
    "    self.validator = validator\n",
    "\n",
    "    def combine_nps_data(self):\n",
    "        \"\"\"\n",
    "        Combines NPS data from all sources in the `sources` dictionary.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If a source is not an NpsDataSource subclass.\n",
    "        \"\"\"\n",
    "\n",
    "        combined = pd.DataFrame()\n",
    "        for name, source in self.sources.items():\n",
    "            raw_data = load_nps_data(source)  # Use separate function for data loading\n",
    "            cleaned_data = clean_nps_data(raw_data)  # Use separate function for data cleaning\n",
    "            data_with_groups = self.convert_csv_to_df(cleaned_data, name)  # Transform cleaned data\n",
    "            combined = pd.concat([combined, data_with_groups])\n",
    "        self.data = combined\n",
    "\n",
    "    def convert_csv_to_df(self, data, source_type):\n",
    "        \"\"\"\n",
    "        Transforms cleaned NPS data by adding source and NPS group columns.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The cleaned NPS data DataFrame.\n",
    "            source_type (str): The source of the NPS responses.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The NPS data DataFrame with source and NPS group columns.\n",
    "        \"\"\"\n",
    "\n",
    "        data[\"source\"] = source_type\n",
    "        data[\"nps_group\"] = data[\"nps_rating\"].apply(self.categorizer.categorize)\n",
    "        self.categorizer.validate_nps_group(data[\"nps_group\"].iloc[0])  # Validate first NPS group\n",
    "        return data\n",
    "\n",
    "    def calculate_nps(self):\n",
    "        \"\"\"\n",
    "        Calculates the overall NPS score.\n",
    "\n",
    "        Assumes the `data` property is already populated with combined NPS data.\n",
    "\n",
    "        Returns:\n",
    "            float: The overall NPS score.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"NPS data is not yet combined. Call combine_nps_data() first.\")\n",
    "\n",
    "        counts = self.data[\"nps_group\"].value_counts()\n",
    "        detractor = counts.get(\"detractor\", 0)\n",
    "        promoter = counts.get(\"promoter\", 0)\n",
    "        total = counts.sum()\n",
    "\n",
    "        return ((promoter - detractor) / total) * 100\n",
    "\n",
    "    def calculate_nps_by_source(self):\n",
    "        \"\"\"\n",
    "        Calculates the NPS score for each source.\n",
    "\n",
    "        Assumes the `data` property is already populated with combined NPS data.\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: A Series with NPS scores for each source.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"NPS data is not yet combined. Call combine_nps_data() first.\")\n",
    "\n",
    "        return self.data.groupby([\"source\"]).apply(self.calculate_nps)\n",
    "\n",
    "    def add_data_source(self, csv_name, source_type):\n",
    "        \"\"\"\n",
    "        Adds a new data source to the `sources` dictionary.\n",
    "\n",
    "        Args:\n",
    "            csv_name (str): The name of the NPS CSV file for the new source.\n",
    "            source_type (str): The type of the new data source.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(source_type, str):\n",
    "            raise TypeError(\"source_type must be a string\")\n",
    "        self.sources[csv_name] = CsvNpsDataSource(csv_name)  # Create CsvNpsDataSource instance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
